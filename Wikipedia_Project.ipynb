{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor or Movie? Classifying Wikipedia Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "For this project, I used data from 20 different Wikipedia articles. The dataset includes the pages for 10 actors and 10 movies. Rather than analyze the text directly, I used part-of-speech (POS) tagging and named entity recognition (NER) tagging on word-tokenized data to obtain numerical measurements of the dataset. More specifically, I counted the amount of each tag for each actor or movie.\n",
    "\n",
    "I used four different machine learning algorithms for classifying the articles. The algorithms I used are Naive Bayes, Decision Tree, and the ensemble method Gradient Boosting.\n",
    "\n",
    "### About the Data Set\n",
    "\n",
    "#### Actors\n",
    "This is an informal project, so I simply chose actors I like. That said, I tried to select a broad range of actors, from B-Movies to blockbusters. My chosen actors are Tessa Thompson, Lupita Nyong'o, Winona Ryder, Amy Poehler, Bruce Campbell, Adam Scott, Nicolas Cage, Tim Curry, Margot Robbie, and Sandra Bullock.\n",
    "#### Movies\n",
    "For each actor above, I chose a movie they appeared in. The movies are \"Sorry to Bother You\", \"Us\", \"Heathers\", \"Wet Hot American Summer\", \"Evil Dead II\", \"Piranha 3D\", \"Con Air\", \"The Rocky Horror Picture Show\", \"I, Tonya\", and \"Miss Congeniality.\"\n",
    "\n",
    "### The Process\n",
    "\n",
    "#### The General Idea\n",
    "If the machine learning models perform well, this suggests that there are quantifiable differences between how actors and movies are written about on Wikipedia. Nota bene, this is a simple, straightforward analysis and it does not prove anything.\n",
    "\n",
    "#### Pre-processing\n",
    "1. Use the [Wikipedia library](https://pypi.org/project/wikipedia/) to gather the content of all Wikipedia pages I will use as my dataset.\n",
    "2. Pre-process this data to remove citations, punctuation, numbers, other extraneous characters, and stopwords and to tokenize by word. \n",
    "\n",
    "\n",
    "#### Analysis\n",
    "1. Tag the word-tokenized data by part-of-speech and by named entity.\n",
    "2. Build a frequency distribution of the top three parts of speech and top three named entities\n",
    "3. Use the counts from each frequency distribution to calculate (for each movie or each actor):\n",
    "    * Mean\n",
    "    * Maximum\n",
    "    * Minimum\n",
    "    * Range\n",
    "    * Standard Deviation\n",
    "4. Select K best features from Movies and from Actors\n",
    "5. Train ML models\n",
    "6. Calculate mean F1 score and mean accuracy score for each model\n",
    "\n",
    "### Results\n",
    "Naive Bayes and Gradient Boosting are tied for both the highest mean F1 score and the highest mean accuracy score (85.3% and 85%, respectively). I would thus consider these two to be the most reliable models. Decision Tree did not perform quite as well, with an F1 of 81.3% and an accuracy of 70%.\n",
    "\n",
    "These scores indicate that there are quantifiable differences between Actor and Movie articles and that these differences can be shown effectively even with a small dataset and limited POS and NER tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and plotting imports\n",
    "#import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK imports\n",
    "from nltk import tokenize, pos_tag, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "# import Stanford NER tagger\n",
    "from nltk.tag import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sources for Stanford NER tagger\n",
    "st = StanfordNERTagger(\n",
    "    '/Users/emmahighland/Desktop/Projects/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "    '/Users/emmahighland/Desktop/Projects/stanford-ner-2018-10-16/stanford-ner.jar',\n",
    "    encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia library -- wrapper for MediaWiki API\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expressions import and patterns\n",
    "# string import to match punctuation\n",
    "import re\n",
    "import string\n",
    "punc = string.punctuation\n",
    "cite = re.compile('(\\[\\d+\\])')\n",
    "year = re.compile('\\(\\d+\\)')\n",
    "headings = re.compile('(=)+')\n",
    "numbers = re.compile('\\d+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn ML imports\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proc(article):\n",
    "    # Tokenize as a sentence\n",
    "    sent = tokenize.sent_tokenize(''.join(article))\n",
    "    # Remove citations, headers (== or ===)\n",
    "    sent_cleaned = [re.sub('(\\s+)',' ',w) and re.sub(cite,'',w) and re.sub(headings,'',w) for w in sent]\n",
    "    \n",
    "    # Tokenize by word\n",
    "    words = tokenize.word_tokenize(' '.join(sent_cleaned))\n",
    "    '''Clean words by removing punctuation, pluralizaton, and numbers.\n",
    "    word_tokenize will list the base word and the pluralization\n",
    "    as separate words (i.e. \"uncle's\" becomes \"uncle\" and \"'s'\").\n",
    "    '''\n",
    "    words_cleaned = [w for w in words if w not in punc and \n",
    "                     re.sub(numbers,'',w) and \n",
    "                     re.sub(year,'',w) and \n",
    "                     w != \"'s\"]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    final_words = [w for w in words_cleaned if w.lower() not in stopwords]\n",
    "    \n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actor Wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tessa = wikipedia.page(\"Tessa Thompson\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lupita = wikipedia.page(\"Lupita Nyong'o\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "amy = wikipedia.page(\"Amy Poehler\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "winona = wikipedia.page(\"Winona Ryder\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bruce = wikipedia.page(\"Bruce Campbell\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = wikipedia.page(\"Adam Scott\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nic = wikipedia.page(\"Nicolas Cage\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "curry = wikipedia.page(\"Tim Curry\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "margot = wikipedia.page(\"Margot Robbie\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandra = wikipedia.page(\"Sandra Bullock\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing actor Wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_list = [tessa,lupita,amy,winona,bruce,adam,nic,curry,margot,sandra]\n",
    "actor_words = []\n",
    "for a in actor_list:\n",
    "    words = pre_proc(a)\n",
    "    actor_words.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Movie Wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stby = wikipedia.page(\"Sorry to Bother You\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = wikipedia.page(\"Us (2019 film)\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "heathers = wikipedia.page(\"Heathers\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "whas = wikipedia.page(\"Wet Hot American Summer\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev2 = wikipedia.page(\"Evil Dead II\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "piranha = wikipedia.page(\"Piranha 3D\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conair = wikipedia.page(\"Con Air\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhps = wikipedia.page(\"The Rocky Horror Picture Show\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "itonya = wikipedia.page(\"I, Tonya\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss = wikipedia.page(\"Miss Congeniality\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing movie Wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_list = [stby,us,heathers,whas,ev2,piranha,conair,rhps,itonya,miss]\n",
    "movie_words = []\n",
    "for m in movie_list:\n",
    "    words = pre_proc(m)\n",
    "    movie_words.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to get frequency distribution for POS and NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freqdist(my_var,tags):\n",
    "    # Universal tagset has simplified parts of speech tagging\n",
    "    var_pos = pos_tag(my_var,tagset='universal')\n",
    "    # Get the top three parts of speech\n",
    "    if tags == 'pos':\n",
    "        var_fd = FreqDist(tag for (word,tag) in var_pos if tag != '.').most_common(3)\n",
    "    elif tags == 'ner':\n",
    "        var_fd = FreqDist(tag for (word,tag) in st.tag(my_var) if tag != 'O').most_common(3)\n",
    "    return var_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create dictionaries to store counts of POS and NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_pos_dict = {}\n",
    "a_ner_dict = {}\n",
    "actor_names = ['Tessa Thompson',\n",
    "               \"Lupita Nyong'o\",\n",
    "               'Winona Ryder',\n",
    "               'Amy Poehler',\n",
    "               'Bruce Campbell',\n",
    "               'Adam Scott',\n",
    "               'Nicolas Cage',\n",
    "               'Tim Curry',\n",
    "              'Margot Robbie',\n",
    "              'Sandra Bullock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dictionaries and \n",
    "m_pos_dict = {}\n",
    "m_ner_dict = {}\n",
    "movie_names = ['Sorry to Bother You',\n",
    "               'Us','Heathers',\n",
    "               'Wet Hot American Summer',\n",
    "               'Evil Dead II',\n",
    "               'Piranha 3D','Con Air',\n",
    "               'The Rocky Horror Picture Show',\n",
    "              'I, Tonya',\n",
    "              'Miss Congeniality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequency distributions to get counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,10):   \n",
    "    pos = get_freqdist(actor_words[x],'pos')\n",
    "    ner = get_freqdist(actor_words[x],'ner')\n",
    "    n = actor_names[x]\n",
    "    a_pos_dict[n] = pos\n",
    "    a_ner_dict[n] = ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,10):   \n",
    "    pos = get_freqdist(movie_words[x],'pos')\n",
    "    ner = get_freqdist(movie_words[x],'ner')\n",
    "    n = movie_names[x]\n",
    "    m_pos_dict[n] = pos\n",
    "    m_ner_dict[n] = ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag count DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Verbs</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Person</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tessa Thompson</th>\n",
       "      <td>419</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>124</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lupita Nyong'o</th>\n",
       "      <td>1422</td>\n",
       "      <td>320</td>\n",
       "      <td>229</td>\n",
       "      <td>253</td>\n",
       "      <td>170</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winona Ryder</th>\n",
       "      <td>1619</td>\n",
       "      <td>389</td>\n",
       "      <td>242</td>\n",
       "      <td>411</td>\n",
       "      <td>159</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amy Poehler</th>\n",
       "      <td>1717</td>\n",
       "      <td>425</td>\n",
       "      <td>280</td>\n",
       "      <td>394</td>\n",
       "      <td>131</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bruce Campbell</th>\n",
       "      <td>927</td>\n",
       "      <td>217</td>\n",
       "      <td>109</td>\n",
       "      <td>182</td>\n",
       "      <td>89</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adam Scott</th>\n",
       "      <td>417</td>\n",
       "      <td>69</td>\n",
       "      <td>46</td>\n",
       "      <td>92</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nicolas Cage</th>\n",
       "      <td>1712</td>\n",
       "      <td>476</td>\n",
       "      <td>285</td>\n",
       "      <td>332</td>\n",
       "      <td>151</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tim Curry</th>\n",
       "      <td>1298</td>\n",
       "      <td>252</td>\n",
       "      <td>145</td>\n",
       "      <td>250</td>\n",
       "      <td>155</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Margot Robbie</th>\n",
       "      <td>845</td>\n",
       "      <td>177</td>\n",
       "      <td>108</td>\n",
       "      <td>265</td>\n",
       "      <td>64</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sandra Bullock</th>\n",
       "      <td>1555</td>\n",
       "      <td>432</td>\n",
       "      <td>274</td>\n",
       "      <td>291</td>\n",
       "      <td>135</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Nouns  Verbs  Adjectives  Person  Organization  Location\n",
       "Tessa Thompson    419     69          40     124            36        16\n",
       "Lupita Nyong'o   1422    320         229     253           170        82\n",
       "Winona Ryder     1619    389         242     411           159        43\n",
       "Amy Poehler      1717    425         280     394           131        53\n",
       "Bruce Campbell    927    217         109     182            89        18\n",
       "Adam Scott        417     69          46      92            45         6\n",
       "Nicolas Cage     1712    476         285     332           151        75\n",
       "Tim Curry        1298    252         145     250           155        48\n",
       "Margot Robbie     845    177         108     265            64        15\n",
       "Sandra Bullock   1555    432         274     291           135        70"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_list = []\n",
    "\n",
    "'''For actors, the top three parts of speech were always\n",
    "    NOUN, VERB, ADJ in that order. The top three named entities\n",
    "    were always PERSON, ORGANIZATION, LOCATION in that order.'''\n",
    "\n",
    "for a in actor_names:\n",
    "    nouncount = a_pos_dict[a][0][1]\n",
    "    verbcount = a_pos_dict[a][1][1]\n",
    "    adjcount = a_pos_dict[a][2][1]\n",
    "    pcount = a_ner_dict[a][0][1]\n",
    "    orgcount = a_ner_dict[a][1][1]\n",
    "    lcount = a_ner_dict[a][2][1]\n",
    "    a_list.append([nouncount,verbcount,adjcount,pcount,orgcount,lcount])\n",
    "    \n",
    "actors_df = pd.DataFrame(a_list,columns=['Nouns',\n",
    "                                        'Verbs','Adjectives',\n",
    "                                        'Person','Organization',\n",
    "                                        'Location'],index=[actor_names])\n",
    "actors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Verbs</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Person</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sorry to Bother You</th>\n",
       "      <td>637</td>\n",
       "      <td>213</td>\n",
       "      <td>144</td>\n",
       "      <td>109</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Us</th>\n",
       "      <td>798</td>\n",
       "      <td>281</td>\n",
       "      <td>141</td>\n",
       "      <td>115</td>\n",
       "      <td>40</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Heathers</th>\n",
       "      <td>1031</td>\n",
       "      <td>315</td>\n",
       "      <td>179</td>\n",
       "      <td>285</td>\n",
       "      <td>98</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wet Hot American Summer</th>\n",
       "      <td>572</td>\n",
       "      <td>132</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evil Dead II</th>\n",
       "      <td>1213</td>\n",
       "      <td>393</td>\n",
       "      <td>244</td>\n",
       "      <td>249</td>\n",
       "      <td>49</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Piranha 3D</th>\n",
       "      <td>827</td>\n",
       "      <td>259</td>\n",
       "      <td>131</td>\n",
       "      <td>228</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Con Air</th>\n",
       "      <td>933</td>\n",
       "      <td>272</td>\n",
       "      <td>148</td>\n",
       "      <td>154</td>\n",
       "      <td>56</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Rocky Horror Picture Show</th>\n",
       "      <td>2384</td>\n",
       "      <td>589</td>\n",
       "      <td>389</td>\n",
       "      <td>419</td>\n",
       "      <td>310</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I, Tonya</th>\n",
       "      <td>1071</td>\n",
       "      <td>348</td>\n",
       "      <td>179</td>\n",
       "      <td>330</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miss Congeniality</th>\n",
       "      <td>517</td>\n",
       "      <td>132</td>\n",
       "      <td>65</td>\n",
       "      <td>96</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Nouns  Verbs  Adjectives  Person  Organization  \\\n",
       "Sorry to Bother You              637    213         144     109            22   \n",
       "Us                               798    281         141     115            40   \n",
       "Heathers                        1031    315         179     285            98   \n",
       "Wet Hot American Summer          572    132          91      91            41   \n",
       "Evil Dead II                    1213    393         244     249            49   \n",
       "Piranha 3D                       827    259         131     228            28   \n",
       "Con Air                          933    272         148     154            56   \n",
       "The Rocky Horror Picture Show   2384    589         389     419           310   \n",
       "I, Tonya                        1071    348         179     330            45   \n",
       "Miss Congeniality                517    132          65      96            24   \n",
       "\n",
       "                               Location  \n",
       "Sorry to Bother You                  17  \n",
       "Us                                   51  \n",
       "Heathers                             17  \n",
       "Wet Hot American Summer              30  \n",
       "Evil Dead II                         30  \n",
       "Piranha 3D                           18  \n",
       "Con Air                              58  \n",
       "The Rocky Horror Picture Show       151  \n",
       "I, Tonya                             18  \n",
       "Miss Congeniality                    50  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_list = []\n",
    "\n",
    "''' For movies, the top three parts of speech were always\n",
    "    NOUN, VERB, ADJ in that order.\n",
    "    Most movies had ORGANIZATION as the second most\n",
    "    common named entity. However, some had\n",
    "    LOCATION instead. I have accounted for both\n",
    "    possibilities.'''\n",
    "\n",
    "for m in movie_names:\n",
    "    nouncount = m_pos_dict[m][0][1]\n",
    "    verbcount = m_pos_dict[m][1][1]\n",
    "    adjcount = m_pos_dict[m][2][1]\n",
    "    pcount = m_ner_dict[m][0][1]\n",
    "    if m_ner_dict[m][1][0] == 'ORGANIZATION':\n",
    "        orgcount = m_ner_dict[m][1][1]\n",
    "    else:\n",
    "        orgcount = m_ner_dict[m][2][1]\n",
    "    if m_ner_dict[m][2][0] == 'LOCATION':\n",
    "        lcount = m_ner_dict[m][2][1]\n",
    "    else:\n",
    "        lcount = m_ner_dict[m][1][1]\n",
    "    m_list.append([nouncount,verbcount,adjcount,pcount,orgcount,lcount])\n",
    "    \n",
    "movies_df = pd.DataFrame(m_list,columns=['Nouns',\n",
    "                                        'Verbs','Adjectives',\n",
    "                                        'Person','Organization',\n",
    "                                        'Location'],index=movie_names)\n",
    "movies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "##### Summary Statistics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_stats(my_df,col_name):\n",
    "    tmp_list = []\n",
    "    tmp_var = my_df[col_name]\n",
    "    for i in range(10):\n",
    "        tmp_list.append(tmp_var[i])\n",
    "    tmp_arr = np.array(tmp_list)\n",
    "    return (tmp_arr.mean(),tmp_arr.min(),tmp_arr.max(),round(tmp_arr.std(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actors Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Verbs</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Person</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>1193.10</td>\n",
       "      <td>282.60</td>\n",
       "      <td>175.80</td>\n",
       "      <td>259.40</td>\n",
       "      <td>113.50</td>\n",
       "      <td>42.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min</th>\n",
       "      <td>417.00</td>\n",
       "      <td>69.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>92.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max</th>\n",
       "      <td>1717.00</td>\n",
       "      <td>476.00</td>\n",
       "      <td>285.00</td>\n",
       "      <td>411.00</td>\n",
       "      <td>170.00</td>\n",
       "      <td>82.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Range</th>\n",
       "      <td>1300.00</td>\n",
       "      <td>407.00</td>\n",
       "      <td>245.00</td>\n",
       "      <td>319.00</td>\n",
       "      <td>134.00</td>\n",
       "      <td>76.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>St.Dev</th>\n",
       "      <td>480.94</td>\n",
       "      <td>141.55</td>\n",
       "      <td>92.16</td>\n",
       "      <td>99.94</td>\n",
       "      <td>47.86</td>\n",
       "      <td>26.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Nouns   Verbs  Adjectives  Person  Organization  Location\n",
       "Mean    1193.10  282.60      175.80  259.40        113.50     42.60\n",
       "Min      417.00   69.00       40.00   92.00         36.00      6.00\n",
       "Max     1717.00  476.00      285.00  411.00        170.00     82.00\n",
       "Range   1300.00  407.00      245.00  319.00        134.00     76.00\n",
       "St.Dev   480.94  141.55       92.16   99.94         47.86     26.28"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list = ['Nouns','Verbs','Adjectives','Person','Organization','Location']\n",
    "a_sum_stats_dict = {}\n",
    "for c in col_list:\n",
    "    me,mi,ma,s = sum_stats(actors_df,c)\n",
    "    a_sum_stats_dict[c] = (me,mi,ma,ma-mi,s)\n",
    "\n",
    "a_sumstats_df = pd.DataFrame(a_sum_stats_dict, index=['Mean','Min','Max','Range','St.Dev'])\n",
    "a_sumstats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Movie Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Verbs</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Person</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>998.30</td>\n",
       "      <td>293.40</td>\n",
       "      <td>171.10</td>\n",
       "      <td>207.60</td>\n",
       "      <td>71.30</td>\n",
       "      <td>44.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min</th>\n",
       "      <td>517.00</td>\n",
       "      <td>132.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>91.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max</th>\n",
       "      <td>2384.00</td>\n",
       "      <td>589.00</td>\n",
       "      <td>389.00</td>\n",
       "      <td>419.00</td>\n",
       "      <td>310.00</td>\n",
       "      <td>151.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Range</th>\n",
       "      <td>1867.00</td>\n",
       "      <td>457.00</td>\n",
       "      <td>324.00</td>\n",
       "      <td>328.00</td>\n",
       "      <td>288.00</td>\n",
       "      <td>134.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>St.Dev</th>\n",
       "      <td>508.95</td>\n",
       "      <td>126.93</td>\n",
       "      <td>86.24</td>\n",
       "      <td>107.24</td>\n",
       "      <td>82.19</td>\n",
       "      <td>38.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Nouns   Verbs  Adjectives  Person  Organization  Location\n",
       "Mean     998.30  293.40      171.10  207.60         71.30     44.00\n",
       "Min      517.00  132.00       65.00   91.00         22.00     17.00\n",
       "Max     2384.00  589.00      389.00  419.00        310.00    151.00\n",
       "Range   1867.00  457.00      324.00  328.00        288.00    134.00\n",
       "St.Dev   508.95  126.93       86.24  107.24         82.19     38.64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_sum_stats_dict = {}\n",
    "for c in col_list:\n",
    "    me,mi,ma,s = sum_stats(movies_df,c)\n",
    "    m_sum_stats_dict[c] = (me,mi,ma,ma-mi,s)\n",
    "\n",
    "m_sumstats_df = pd.DataFrame(m_sum_stats_dict, index=['Mean','Min','Max','Range','St.Dev'])\n",
    "m_sumstats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary statistics interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these summary statistics, it appears that the Movies data frame shows more variation overall. The range of the \"nouns\" tag count is higher here than for Actors. The amount of \"organization\" and \"location\" tags are much more variable for Movies than Actors, as shown by the broader range and higher standard deviation. In both cases, the ranges and standard deviations are approximately doubled for Movies relative to Actors.\n",
    "\n",
    "There are also some similarities between the two categories. The \"person\" tag is most similar between Movies and Actors overall. The average amounts of nouns, verbs, adjectives, persons, organizations, and locations are comparable between Actors and Movies. In other words, there is a fairly consistent ratio. The amount of Nouns per individual actor or movie is the most variable for both categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My suspicion is that the most telling features are \"nouns\",\"person\",\"organization\",and \"location\". I am testing this using the k best algorithm for feature selection. Since this is a small data set, I am going to test the ML models with both the k best features and the full set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_np = actors_df.to_numpy()\n",
    "movies_np = movies_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 419,   69,   40,  124,   36,   16],\n",
       "       [1422,  320,  229,  253,  170,   82],\n",
       "       [1619,  389,  242,  411,  159,   43],\n",
       "       [1717,  425,  280,  394,  131,   53],\n",
       "       [ 927,  217,  109,  182,   89,   18],\n",
       "       [ 417,   69,   46,   92,   45,    6],\n",
       "       [1712,  476,  285,  332,  151,   75],\n",
       "       [1298,  252,  145,  250,  155,   48],\n",
       "       [ 845,  177,  108,  265,   64,   15],\n",
       "       [1555,  432,  274,  291,  135,   70],\n",
       "       [ 637,  213,  144,  109,   22,   17],\n",
       "       [ 798,  281,  141,  115,   40,   51],\n",
       "       [1031,  315,  179,  285,   98,   17],\n",
       "       [ 572,  132,   91,   91,   41,   30],\n",
       "       [1213,  393,  244,  249,   49,   30],\n",
       "       [ 827,  259,  131,  228,   28,   18],\n",
       "       [ 933,  272,  148,  154,   56,   58],\n",
       "       [2384,  589,  389,  419,  310,  151],\n",
       "       [1071,  348,  179,  330,   45,   18],\n",
       "       [ 517,  132,   65,   96,   24,   50]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Create Numpy array to be the key for ML training\n",
    "The goal is to train the ML algorithms to match this key.\n",
    "1 denotes actor, 0 denotes movie'''\n",
    "goal = np.array([1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0])\n",
    "'''Create a Numpy array that combines actors and movies.\n",
    "The first 10 rows are actors (1) and the last 10 are\n",
    "movies (0). This corresponds to the previous Numpy array.'''\n",
    "m_a_np = np.concatenate((actors_np,movies_np))\n",
    "m_a_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 419,   69,  124,   36],\n",
       "       [1422,  320,  253,  170],\n",
       "       [1619,  389,  411,  159],\n",
       "       [1717,  425,  394,  131],\n",
       "       [ 927,  217,  182,   89],\n",
       "       [ 417,   69,   92,   45],\n",
       "       [1712,  476,  332,  151],\n",
       "       [1298,  252,  250,  155],\n",
       "       [ 845,  177,  265,   64],\n",
       "       [1555,  432,  291,  135],\n",
       "       [ 637,  213,  109,   22],\n",
       "       [ 798,  281,  115,   40],\n",
       "       [1031,  315,  285,   98],\n",
       "       [ 572,  132,   91,   41],\n",
       "       [1213,  393,  249,   49],\n",
       "       [ 827,  259,  228,   28],\n",
       "       [ 933,  272,  154,   56],\n",
       "       [2384,  589,  419,  310],\n",
       "       [1071,  348,  330,   45],\n",
       "       [ 517,  132,   96,   24]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_a_new = SelectKBest(chi2, k=4).fit_transform(m_a_np, goal)\n",
    "m_a_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I anticipated that \"nouns\",\"person\",\"organization\", and \"location\" would be the most telling. The best 4 features are actually \"nouns\", \"verbs\", \"person\", and \"organization\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ML Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "clf_nb = MultinomialNB()\n",
    "# Decision Tree\n",
    "clf_dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Ensemble methods\n",
    "# Random forest\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, \n",
    "                                max_features=\"sqrt\",\n",
    "                                max_depth=None,\n",
    "                                min_samples_split=2, \n",
    "                                random_state=0)\n",
    "# Gradient boost\n",
    "clf_gb = GradientBoostingClassifier(n_estimators=100, \n",
    "                                    learning_rate=1.0,\n",
    "                                    max_depth=None, \n",
    "                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model Training and Evaluation using K best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes, mean F1 score and mean accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8533333333333333, 0.85)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_nb_scores_f1 = cross_val_score(clf_nb, m_a_new, goal, cv=5,scoring='f1')\n",
    "k_nb_scores_acc = cross_val_score(clf_nb,m_a_new,goal,cv=5,scoring='accuracy')\n",
    "k_nb_scores_f1.mean(),k_nb_scores_acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree, mean F1 and mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8133333333333335, 0.7)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_dt_scores_f1 = cross_val_score(clf_dt, m_a_new, goal, cv=5,scoring='f1')\n",
    "k_dt_scores_acc = cross_val_score(clf_dt,m_a_new,goal,cv=5,scoring='accuracy')\n",
    "k_dt_scores_f1.mean(),k_dt_scores_acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting, mean F1 and mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8533333333333333, 0.85)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_gb_scores_f1 = cross_val_score(clf_gb, m_a_new, goal, cv=5,scoring='f1')\n",
    "k_gb_scores_acc = cross_val_score(clf_gb,m_a_new,goal,cv=5,scoring='accuracy')\n",
    "k_gb_scores_f1.mean(),k_gb_scores_acc.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
